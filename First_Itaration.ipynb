{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piras2024/quantized-dql-mountaincar/blob/main/First_Itaration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "PCXLfbV5-ME5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efe2157c"
      },
      "source": [
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_actions, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(input_dim, 12),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(12, 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(8, num_actions)\n",
        "            )\n",
        "\n",
        "        # Initialize FC layer weights using He initialization\n",
        "        for layer in [self.FC]:\n",
        "            for module in layer:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.FC(x)\n",
        "        return Q\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f92191c5"
      },
      "source": [
        "# MountainCar Deep Q-Learning\n",
        "class MountainCarDQL():\n",
        "\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    def __init__(self, learning_rate_a=75e-5, discount_factor_g=0.96, network_sync_rate=100, replay_memory_size=100000, mini_batch_size=64, num_discrete_actions=10, seed=None):\n",
        "        self.learning_rate_a = learning_rate_a\n",
        "        self.discount_factor_g = discount_factor_g\n",
        "        self.network_sync_rate = network_sync_rate\n",
        "        self.replay_memory_size = replay_memory_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.num_discrete_actions = num_discrete_actions\n",
        "        self.seed = seed\n",
        "\n",
        "        if self.seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(self.seed)\n",
        "            torch.manual_seed(self.seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(self.seed)\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "    # Train the environment\n",
        "    def train(self, episodes, render=False):\n",
        "        # Create MountainCarContinuous instance\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_train_video', episode_trigger=lambda x: x % 1000 == 0) # Record every 1000 episodes during training\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "        epsilon = 1 # 1 = 100% random actions\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        target_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "        best_rewards=-1000 # Adjusted initial best_rewards for continuous env\n",
        "        goal_reached=False\n",
        "\n",
        "        for i in range(episodes):\n",
        "\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent falls in hole or reached goal\n",
        "            truncated = False\n",
        "\n",
        "            rewards = 0\n",
        "\n",
        "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while(not terminated and not truncated): # The reward threshold might need adjustment for continuous env\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action (index for discrete actions) uniformly\n",
        "                    action_index = random.randrange(self.num_discrete_actions)\n",
        "\n",
        "                else:\n",
        "                    # select best action (index for discrete actions)\n",
        "                    with torch.no_grad():\n",
        "                        # Use the continuous state as input and get the index of the best discrete action\n",
        "                        action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                new_state,reward,terminated,truncated,_ = env.step([action])\n",
        "\n",
        "                # Add a small negative reward at each timestep to discourage staying in the valley\n",
        "                reward -= 1\n",
        "\n",
        "                # Accumulate reward\n",
        "                rewards += reward\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action_index, new_state, reward, terminated)) # Store action_index, not continuous action value\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(rewards)\n",
        "\n",
        "            # Log reward per episode to wandb\n",
        "            wandb.log({\"reward_per_episode\": rewards}, step=i)\n",
        "\n",
        "\n",
        "            # Check if goal was reached\n",
        "            if(terminated):\n",
        "                goal_reached = True\n",
        "\n",
        "            # Graph training progress\n",
        "            if(i!=0 and i%1000==0):\n",
        "                print(f'Episode {i} Epsilon {epsilon}')\n",
        "\n",
        "                #self.plot_progress(rewards_per_episode, epsilon_history)\n",
        "                #torch.save(policy_dqn.state_dict(), f\"mountaincar_autosave_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            if rewards>best_rewards:\n",
        "                best_rewards = rewards\n",
        "                print(f'Best rewards so far: {best_rewards}')\n",
        "                # Save policy\n",
        "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            # Check if enough experience has been collected AND goal was reached in this episode\n",
        "            if len(memory)>self.mini_batch_size and goal_reached: # Restored 'and goal_reached'\n",
        "                mini_batch = memory.sample(self.mini_batch_size) # Use mini_batch_size for sampling\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                # Decay epsilon\n",
        "                epsilon = max(epsilon - 1/episodes, 0)\n",
        "                epsilon_history.append(epsilon)\n",
        "\n",
        "                # Log epsilon to wandb\n",
        "                wandb.log({\"epsilon\": epsilon}, step=i)\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
        "        # Create new graph\n",
        "        plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
        "        # for x in range(len(rewards_per_episode)):\n",
        "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
        "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.plot(sum_rewards)\n",
        "        plt.plot(rewards_per_episode)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        plt.savefig('mountaincar_dql.png')\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action_index, new_state, reward, terminated in mini_batch: # Use action_index\n",
        "\n",
        "            if terminated:\n",
        "                # Agent receive reward of 0 for reaching goal.\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            # Use the continuous state as input\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            # Use the continuous state as input\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
        "            # Adjust the specific action (index) to the target that was just calculated\n",
        "            target_q[action_index] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts a state (position, velocity) to tensor representation for continuous observation space.\n",
        "    Example:\n",
        "    Input = (0.3, -0.03)\n",
        "    Return = tensor([0.3, -0.03])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
        "        # The state is already a NumPy array [position, velocity]\n",
        "        # Convert it directly to a PyTorch FloatTensor\n",
        "        return torch.FloatTensor(state)\n",
        "\n",
        "    # Run the environment with the learned policy\n",
        "    def test(self, episodes, model_filepath):\n",
        "        # Create FrozenLake instance\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_test_video', episode_trigger=lambda x: True) # Record every episode\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        #if self.seed is not None:\n",
        "        #    env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "\n",
        "        num_states = env.observation_space.shape[0]\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "\n",
        "        # Divide position and velocity into segments - No longer needed for continuous observation space\n",
        "        # self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
        "        # self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
        "\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        total_test_rewards = 0\n",
        "        test_rewards_list = []\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent falls in hole or reached goal\n",
        "            truncated = False       # True when agent takes more than 200 actions\n",
        "            rewards = 0\n",
        "\n",
        "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action (index)\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input and get the index of the best discrete action\n",
        "                    action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                state,reward,terminated,truncated,_ = env.step([action])\n",
        "                rewards += reward\n",
        "\n",
        "            total_test_rewards += rewards\n",
        "            test_rewards_list.append(rewards)\n",
        "\n",
        "            # Check if the goal was reached (terminated without truncation)\n",
        "            # MountainCarContinuous-v0 terminates when the flag is reached (position >= 0.5)\n",
        "            if terminated:\n",
        "                print(f\"Episode {i+1}: Goal Reached! Reward: {rewards}\")\n",
        "            elif truncated:\n",
        "                print(f\"Episode {i+1}: Episode truncated (did not reach goal). Reward: {rewards}\")\n",
        "            else: # This case should not happen in MountainCarContinuous if not truncated\n",
        "                 print(f\"Episode {i+1}: Episode terminated unexpectedly. Reward: {rewards}\")\n",
        "\n",
        "        # Calculate and Log average test reward to wandb\n",
        "        if episodes > 0:\n",
        "            avg_test_reward = total_test_rewards / episodes\n",
        "            wandb.log({\"average_test_reward\": avg_test_reward})\n",
        "            print(f\"Average test reward over {episodes} episodes: {avg_test_reward}\")\n",
        "        else:\n",
        "            print(\"No test episodes run.\")\n",
        "\n",
        "        # Log test videos to wandb\n",
        "        # Assuming videos are saved in 'mountaincar_test_video' directory\n",
        "        # Wandb can log video files directly.\n",
        "        # We need to find the video files generated during this test run.\n",
        "        # The RecordVideo wrapper names videos based on the episode index.\n",
        "        video_files = glob.glob('mountaincar_test_video/rl-video-episode-*.mp4')\n",
        "        if video_files:\n",
        "            print(f\"Logging {len(video_files)} test videos to wandb.\")\n",
        "            for video_file in video_files:\n",
        "                wandb.log({\"test_video\": wandb.Video(video_file)})\n",
        "        else:\n",
        "            print(\"No test videos found to log.\")\n",
        "\n",
        "\n",
        "        env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3517fdf4",
        "outputId": "58395278-df3c-44f2-b6f0-d6b4aaace100"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizzazione spazio delle azioni in 3 azioni"
      ],
      "metadata": {
        "id": "GwZV4d4eSMrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=3, learning_rate_a=0.001, discount_factor_g=0.99, seed=2025)\n",
        "\n",
        "mountaincar.train(20000, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "id": "tTds2tqvwwBX",
        "outputId": "f39e3634-55ea-4302-a2e5-df9f25c72bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best rewards so far: -874.0000000000093\n",
            "Best rewards so far: -661.9000000000067\n",
            "Best rewards so far: -549.8000000000051\n",
            "Best rewards so far: -465.40000000000384\n",
            "Best rewards so far: -379.80000000000257\n",
            "Episode 1000 Epsilon 0.9500500000000055\n",
            "Episode 2000 Epsilon 0.900050000000011\n",
            "Best rewards so far: -294.2000000000013\n",
            "Episode 3000 Epsilon 0.8500500000000165\n",
            "Episode 4000 Epsilon 0.800050000000022\n",
            "Episode 5000 Epsilon 0.7500500000000275\n",
            "Episode 6000 Epsilon 0.700050000000033\n",
            "Best rewards so far: -230.20000000000036\n",
            "Episode 7000 Epsilon 0.6500500000000385\n",
            "Episode 8000 Epsilon 0.600050000000044\n",
            "Episode 9000 Epsilon 0.5500500000000496\n",
            "Episode 10000 Epsilon 0.5000500000000551\n",
            "Episode 11000 Epsilon 0.45005000000006057\n",
            "Best rewards so far: -183.1999999999997\n",
            "Best rewards so far: -104.19999999999956\n",
            "Episode 12000 Epsilon 0.4000500000000661\n",
            "Best rewards so far: -99.69999999999962\n",
            "Best rewards so far: -89.69999999999965\n",
            "Episode 13000 Epsilon 0.3500500000000716\n",
            "Episode 14000 Epsilon 0.3000500000000771\n",
            "Episode 15000 Epsilon 0.2500500000000826\n",
            "Best rewards so far: -60.79999999999984\n",
            "Best rewards so far: -53.19999999999979\n",
            "Episode 16000 Epsilon 0.2000500000000881\n",
            "Best rewards so far: -36.39999999999978\n",
            "Episode 17000 Epsilon 0.1500500000000936\n",
            "Episode 18000 Epsilon 0.10005000000009912\n",
            "Episode 19000 Epsilon 0.050050000000102894\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2363e5"
      },
      "source": [
        "**Reasoning**:\n",
        "The training has completed. Now, execute the test cell to evaluate the performance of the trained agent. Make sure to update the model filepath to the latest saved model if necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8147d03",
        "outputId": "fcf2cccf-69a5-45c7-e14b-1b088a6e173e"
      },
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_16009.pt\n",
            "Episode 1: Goal Reached! Reward: 86.70000000000003\n",
            "Episode 2: Goal Reached! Reward: 90.20000000000002\n",
            "Episode 3: Goal Reached! Reward: 90.20000000000002\n",
            "Episode 4: Goal Reached! Reward: 90.40000000000002\n",
            "Episode 5: Goal Reached! Reward: 88.30000000000003\n",
            "Episode 6: Goal Reached! Reward: 86.80000000000003\n",
            "Episode 7: Goal Reached! Reward: 90.20000000000002\n",
            "Episode 8: Goal Reached! Reward: 90.10000000000002\n",
            "Episode 9: Goal Reached! Reward: 86.80000000000003\n",
            "Episode 10: Goal Reached! Reward: 86.60000000000004\n",
            "Episode 11: Goal Reached! Reward: 86.80000000000003\n",
            "Episode 12: Goal Reached! Reward: 88.00000000000003\n",
            "Episode 13: Goal Reached! Reward: 88.00000000000003\n",
            "Episode 14: Goal Reached! Reward: 90.40000000000002\n",
            "Episode 15: Goal Reached! Reward: 86.40000000000003\n",
            "Episode 16: Goal Reached! Reward: 88.40000000000002\n",
            "Episode 17: Goal Reached! Reward: 86.30000000000004\n",
            "Episode 18: Goal Reached! Reward: 86.80000000000003\n",
            "Episode 19: Goal Reached! Reward: 88.30000000000003\n",
            "Episode 20: Goal Reached! Reward: 88.30000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fase di test si osserva un reward massimo di 90.2\n"
      ],
      "metadata": {
        "id": "5HVo0Up8gODL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizzazione dello spazio delle azioni in 10 azioni"
      ],
      "metadata": {
        "id": "O5jO8tARgZAU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "8c8490b9",
        "outputId": "b09e727d-7c64-4775-ffac-adba8db73f0f"
      },
      "source": [
        "import wandb\n",
        "\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": 0.001,\n",
        "    \"discount_factor_g\": 0.99,\n",
        "    \"network_sync_rate\": 100,\n",
        "    \"replay_memory_size\": 100000,\n",
        "    \"mini_batch_size\": 64,\n",
        "    \"num_discrete_actions\": 10,\n",
        "    \"seed\": 2025\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_10_Actions\", config=hyperparameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ancient-wind-1</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/we1axs1u' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/we1axs1u</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250723_134250-we1axs1u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250723_135020-vc8wx32n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/vc8wx32n' target=\"_blank\">MountainCar_DQL_Run_10_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/vc8wx32n' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/vc8wx32n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/vc8wx32n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x781591279950>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kK93DcuggKV",
        "outputId": "258056f8-8d4a-4775-d952-f213e2fa4324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: mountaincar_dql_2222.pt\n",
            "Deleted: mountaincar_autosave_dql_7000.pt\n",
            "Deleted: mountaincar_autosave_dql_3000.pt\n",
            "Deleted: mountaincar_autosave_dql_4000.pt\n",
            "Deleted: mountaincar_autosave_dql_8000.pt\n",
            "Deleted: mountaincar_autosave_dql_6000.pt\n",
            "Deleted: mountaincar_dql_15010.pt\n",
            "Deleted: mountaincar_autosave_dql_19000.pt\n",
            "Deleted: mountaincar_dql_11636.pt\n",
            "Deleted: mountaincar_dql_12679.pt\n",
            "Deleted: mountaincar_dql_12363.pt\n",
            "Deleted: mountaincar_autosave_dql_9000.pt\n",
            "Deleted: mountaincar_autosave_dql_5000.pt\n",
            "Deleted: mountaincar_dql_238.pt\n",
            "Deleted: mountaincar_autosave_dql_18000.pt\n",
            "Deleted: mountaincar_autosave_dql_14000.pt\n",
            "Deleted: mountaincar_dql_6427.pt\n",
            "Deleted: mountaincar_autosave_dql_1000.pt\n",
            "Deleted: mountaincar_dql_15013.pt\n",
            "Deleted: mountaincar_dql_143.pt\n",
            "Deleted: mountaincar_dql_11534.pt\n",
            "Deleted: mountaincar_autosave_dql_13000.pt\n",
            "Deleted: mountaincar_autosave_dql_11000.pt\n",
            "Deleted: mountaincar_autosave_dql_16000.pt\n",
            "Deleted: mountaincar_autosave_dql_15000.pt\n",
            "Deleted: mountaincar_dql_65.pt\n",
            "Deleted: mountaincar_autosave_dql_2000.pt\n",
            "Deleted: mountaincar_autosave_dql_10000.pt\n",
            "Deleted: mountaincar_dql_19.pt\n",
            "Deleted: mountaincar_autosave_dql_12000.pt\n",
            "Deleted: mountaincar_dql_1.pt\n",
            "Deleted: mountaincar_autosave_dql_17000.pt\n",
            "Deleted: mountaincar_dql_16009.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=10, learning_rate_a=0.001, discount_factor_g=0.99, seed=2025)\n",
        "\n",
        "mountaincar.train(20000, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIH6DcDHglbP",
        "outputId": "0360f88e-b2bd-4b9a-bbbe-6d04737d5b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_train_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best rewards so far: -929.0691370393114\n",
            "Best rewards so far: -779.5481491676148\n",
            "Best rewards so far: -627.4345687779411\n",
            "Episode 1000 Epsilon 0.9505000000000055\n",
            "Best rewards so far: -585.0444452457979\n",
            "Best rewards so far: -408.5888894809612\n",
            "Best rewards so far: -307.18642029247087\n",
            "Episode 2000 Epsilon 0.900500000000011\n",
            "Episode 3000 Epsilon 0.8505000000000165\n",
            "Episode 4000 Epsilon 0.800500000000022\n",
            "Episode 5000 Epsilon 0.7505000000000275\n",
            "Episode 6000 Epsilon 0.700500000000033\n",
            "Episode 7000 Epsilon 0.6505000000000385\n",
            "Episode 8000 Epsilon 0.600500000000044\n",
            "Episode 9000 Epsilon 0.5505000000000495\n",
            "Episode 10000 Epsilon 0.500500000000055\n",
            "Episode 11000 Epsilon 0.4505000000000605\n",
            "Episode 12000 Epsilon 0.400500000000066\n",
            "Best rewards so far: -208.33950662642272\n",
            "Best rewards so far: -110.01604971326367\n",
            "Best rewards so far: -102.8000003019963\n",
            "Episode 13000 Epsilon 0.35050000000007153\n",
            "Episode 14000 Epsilon 0.30050000000007704\n",
            "Episode 15000 Epsilon 0.25050000000008255\n",
            "Episode 16000 Epsilon 0.20050000000008805\n",
            "Episode 17000 Epsilon 0.15050000000009356\n",
            "Episode 18000 Epsilon 0.10050000000009907\n",
            "Episode 19000 Epsilon 0.05050000000010291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ8o7UO1viLj",
        "outputId": "447ac18f-7007-4e45-8313-e302e901af60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_12870.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_test_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Goal Reached! Reward: 93.72592558772477\n",
            "Episode 2: Goal Reached! Reward: 93.74938236048193\n",
            "Episode 3: Goal Reached! Reward: 93.81111076540418\n",
            "Episode 4: Goal Reached! Reward: 94.80493795444937\n",
            "Episode 5: Goal Reached! Reward: 95.71728371146284\n",
            "Episode 6: Goal Reached! Reward: 95.51357999363063\n",
            "Episode 7: Goal Reached! Reward: 95.85925902834644\n",
            "Episode 8: Goal Reached! Reward: 93.69876507461808\n",
            "Episode 9: Goal Reached! Reward: 93.71975275425265\n",
            "Episode 10: Goal Reached! Reward: 95.89876520177464\n",
            "Episode 11: Goal Reached! Reward: 93.81111076540418\n",
            "Episode 12: Goal Reached! Reward: 95.92098742929505\n",
            "Episode 13: Goal Reached! Reward: 93.77037001627464\n",
            "Episode 14: Goal Reached! Reward: 93.75802435992678\n",
            "Episode 15: Goal Reached! Reward: 95.92098742929505\n",
            "Episode 16: Goal Reached! Reward: 93.76049347424214\n",
            "Episode 17: Goal Reached! Reward: 93.71851815735853\n",
            "Episode 18: Goal Reached! Reward: 95.8296293956262\n",
            "Episode 19: Goal Reached! Reward: 93.84197496587849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Goal Reached! Reward: 93.72716013163698\n",
            "Average test reward over 20 episodes: 94.52790092785419\n",
            "Logging 21 test videos to wandb.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con 10 Azioni si osserva un avg reward di 94.5 su 20 tests"
      ],
      "metadata": {
        "id": "UgDvAl3KwLNR"
      }
    }
  ]
}