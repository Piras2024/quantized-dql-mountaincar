{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piras2024/quantized-dql-mountaincar/blob/main/Mountain_car_continuous_dql_quantized_action_space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "PCXLfbV5-ME5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efe2157c"
      },
      "source": [
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_actions, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(input_dim, 12),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(12, 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(8, num_actions)\n",
        "            )\n",
        "\n",
        "        # Initialize FC layer weights using He initialization\n",
        "        for layer in [self.FC]:\n",
        "            for module in layer:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.FC(x)\n",
        "        return Q\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30b0af39"
      },
      "source": [
        "# MountainCar Deep Q-Learning\n",
        "class MountainCarDQL():\n",
        "\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    def __init__(self, learning_rate_a=75e-5, discount_factor_g=0.96, network_sync_rate=100, replay_memory_size=100000, mini_batch_size=64, num_discrete_actions=10, seed=None, lr_decay_gamma=0.9, lr_step_size=1000, epsilon_decay_c1=1000, epsilon_decay_c2=1000, epsilon_decay_rate=0.0001, epsilon_min=0.01):\n",
        "        self.learning_rate_a = learning_rate_a\n",
        "        self.discount_factor_g = discount_factor_g\n",
        "        self.network_sync_rate = network_sync_rate\n",
        "        self.replay_memory_size = replay_memory_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.num_discrete_actions = num_discrete_actions\n",
        "        self.seed = seed\n",
        "        self.lr_decay_gamma = lr_decay_gamma # learning rate decay\n",
        "        self.lr_step_size = lr_step_size     # learning rate decay\n",
        "        self.epsilon_decay_c1 = epsilon_decay_c1 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_c2 = epsilon_decay_c2 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate # exponential epsilon decay\n",
        "        self.epsilon_min = epsilon_min # minimum epsilon value\n",
        "\n",
        "        #To select which epsilon decay strategy is going to be used\n",
        "        self.linearDecay = False\n",
        "        self.hyperbolicDecay = True\n",
        "        self.exponentialDecay = False\n",
        "\n",
        "        #seed evrithing for reproducability\n",
        "        if self.seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(self.seed)\n",
        "            torch.manual_seed(self.seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(self.seed)\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "    # Train the environment\n",
        "    def train(self, episodes, render=False):\n",
        "        # Create MountainCarContinuous instance\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_train_video', episode_trigger=lambda x: x % 1000 == 0) # Record every 1000 episodes during training\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "        epsilon = 1 # Initial epsilon\n",
        "\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        target_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # Learning rate scheduler - for learning rate decay\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.lr_step_size, gamma=self.lr_decay_gamma)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "        best_rewards=-200 # Adjusted initial best_rewards for continuous env\n",
        "        goal_reached=False #it is used to start training the network when the goal is reached at lest once in one of the episodes\n",
        "\n",
        "        for i in range(episodes):\n",
        "\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent reached goal\n",
        "            truncated = False\n",
        "\n",
        "            rewards = 0\n",
        "\n",
        "            # Agent navigates map until it falls into reaches goal (terminated), or the lenght of the episode is 999 (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action (index for discrete actions) uniformly\n",
        "                    action_index = random.randrange(self.num_discrete_actions)\n",
        "\n",
        "                else:\n",
        "                    # select best action (index for discrete actions)\n",
        "                    with torch.no_grad():\n",
        "                        # Use the continuous state as input and get the index of the best discrete action\n",
        "                        action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                new_state,reward,terminated,truncated,_ = env.step([action])\n",
        "\n",
        "                # Add a small negative reward at each timestep to discourage staying in the valley\n",
        "                reward -= 1\n",
        "\n",
        "                # Accumulate reward\n",
        "                rewards += reward\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action_index, new_state, reward, terminated)) # Store action_index, not continuous action value\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(rewards)\n",
        "\n",
        "            # Log reward per episode to wandb\n",
        "            wandb.log({\"reward_per_episode\": rewards}, step=i)\n",
        "\n",
        "\n",
        "            # Check if goal was reached\n",
        "            if(terminated):\n",
        "                goal_reached = True\n",
        "\n",
        "            # Graph training progress\n",
        "            #if(i!=0 and i%1000==0):\n",
        "                #print(f'Episode {i} Epsilon {epsilon}')\n",
        "\n",
        "                #self.plot_progress(rewards_per_episode, epsilon_history)\n",
        "                #torch.save(policy_dqn.state_dict(), f\"mountaincar_autosave_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            if rewards>best_rewards:\n",
        "                best_rewards = rewards\n",
        "                print(f'Best rewards so far: {best_rewards}')\n",
        "                # Save policy\n",
        "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            # Check if enough experience has been collected AND goal was reached\n",
        "            if len(memory)>self.mini_batch_size and goal_reached:\n",
        "                mini_batch = memory.sample(self.mini_batch_size) # Use mini_batch_size for sampling\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                #decay epsilon\n",
        "                if(self.linearDecay):\n",
        "                  epsilon = max(epsilon - 1/episodes, self.epsilon_min)\n",
        "                elif(self.hyperbolicDecay):\n",
        "                  epsilon = max(self.epsilon_decay_c1 / (self.epsilon_decay_c2 + i), self.epsilon_min)\n",
        "                elif(self.exponentialDecay):\n",
        "                  epsilon = self.epsilon_min + (1 - self.epsilon_min) * math.exp(-self.epsilon_decay_rate * i)\n",
        "\n",
        "                epsilon_history.append(epsilon)\n",
        "                # Log epsilon to wandb\n",
        "                wandb.log({\"epsilon\": epsilon}, step=i)\n",
        "\n",
        "                # Step the learning rate scheduler - for learning rate decay\n",
        "                self.scheduler.step()\n",
        "                # Log current learning rate to wandb\n",
        "                wandb.log({\"learning_rate\": self.optimizer.param_groups[0]['lr']}, step=i)\n",
        "\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Save the final model\n",
        "        #torch.save(policy_dqn.state_dict(), \"mountaincar_dql_final.pt\")\n",
        "        #print(\"Final model saved as mountaincar_dql_final.pt\")\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "    #def plot_progress(self, rewards_per_episode, epsilon_history):\n",
        "        # Create new graph\n",
        "        #plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
        "        # for x in range(len(rewards_per_episode)):\n",
        "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
        "        #plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.plot(sum_rewards)\n",
        "        #plt.plot(rewards_per_episode)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        #plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        #plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        #plt.savefig('mountaincar_dql.png')\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action_index, new_state, reward, terminated in mini_batch: # Use action_index\n",
        "\n",
        "            if terminated:\n",
        "                # Agent receive reward of 100 for reaching goal.\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            # Use the continuous state as input\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            # Use the continuous state as input\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
        "            # Adjust the specific action (index) to the target that was just calculated\n",
        "            target_q[action_index] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts a state (position, velocity) to tensor representation for continuous observation space.\n",
        "    Example:\n",
        "    Input = (0.3, -0.03)\n",
        "    Return = tensor([0.3, -0.03])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
        "        # The state is already a NumPy array [position, velocity]\n",
        "        # Convert it directly to a PyTorch FloatTensor\n",
        "        return torch.FloatTensor(state)\n",
        "\n",
        "    # Run the environment with the learned policy\n",
        "    def test(self, episodes, model_filepath):\n",
        "\n",
        "\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_test_video', episode_trigger=lambda x: True) # Record every episode\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "\n",
        "        num_states = env.observation_space.shape[0]\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        total_test_rewards = 0\n",
        "        test_rewards_list = []\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent reached goal\n",
        "            truncated = False\n",
        "            rewards = 0\n",
        "\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action (index)\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input and get the index of the best discrete action\n",
        "                    action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                state,reward,terminated,truncated,_ = env.step([action])\n",
        "                rewards += reward\n",
        "\n",
        "            total_test_rewards += rewards\n",
        "            test_rewards_list.append(rewards)\n",
        "\n",
        "            # Check if the goal was reached (terminated without truncation)\n",
        "            # MountainCarContinuous-v0 terminates when the flag is reached\n",
        "            if terminated:\n",
        "                print(f\"Episode {i+1}: Goal Reached! Reward: {rewards}\")\n",
        "            elif truncated:\n",
        "                print(f\"Episode {i+1}: Episode truncated (did not reach goal). Reward: {rewards}\")\n",
        "            else: # This case should not happen in MountainCarContinuous if not truncated\n",
        "                 print(f\"Episode {i+1}: Episode terminated unexpectedly. Reward: {rewards}\")\n",
        "\n",
        "        # Calculate and Log average test reward to wandb\n",
        "        if episodes > 0:\n",
        "            avg_test_reward = total_test_rewards / episodes\n",
        "            wandb.log({\"average_test_reward\": avg_test_reward})\n",
        "            print(f\"Average test reward over {episodes} episodes: {avg_test_reward}\")\n",
        "        else:\n",
        "            print(\"No test episodes run.\")\n",
        "\n",
        "        # Log test videos to wandb\n",
        "        # Assuming videos are saved in 'mountaincar_test_video' directory\n",
        "        # Wandb can log video files directly.\n",
        "        # We need to find the video files generated during this test run.\n",
        "        # The RecordVideo wrapper names videos based on the episode index.\n",
        "        video_files = glob.glob('mountaincar_test_video/rl-video-episode-*.mp4')\n",
        "        if video_files:\n",
        "            print(f\"Logging {len(video_files)} test videos to wandb.\")\n",
        "            for video_file in video_files:\n",
        "                wandb.log({\"test_video\": wandb.Video(video_file)})\n",
        "        else:\n",
        "            print(\"No test videos found to log.\")\n",
        "\n",
        "\n",
        "        env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3517fdf4",
        "outputId": "97313f21-9593-4a0c-e2b1-b1b19c6c13ff"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: mountaincar_dql_7657.pt\n",
            "Deleted: mountaincar_dql_14216.pt\n",
            "Deleted: mountaincar_dql_10372.pt\n",
            "Deleted: mountaincar_dql_13156.pt\n",
            "Deleted: mountaincar_dql_4207.pt\n",
            "Deleted: mountaincar_dql_14134.pt\n",
            "Deleted: mountaincar_dql_11241.pt\n",
            "Deleted: mountaincar_dql_1585.pt\n",
            "Deleted: mountaincar_dql_9408.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizzazione spazio delle azioni in 3 azioni"
      ],
      "metadata": {
        "id": "GwZV4d4eSMrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "num_discrete_actions=3\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.997\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.9\n",
        "lr_step_size=500\n",
        "epsilon_decay_c1=1000\n",
        "epsilon_decay_c2=1000\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    \"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    \"epsilon_decay_c2\": epsilon_decay_c2\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_3_Actions\", config=hyperparameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "S40qAyRVZ2pf",
        "outputId": "9e971ae9-cdd5-4dc1-8580-aa1b88dac175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>█▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward_per_episode</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▁▇▁▄▆▁██▁▁▁▇▁▁▁▁▁▁▂▁▂▁▆▁▁▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.5</td></tr><tr><td>learning_rate</td><td>0.01</td></tr><tr><td>reward_per_episode</td><td>-1083.1</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MountainCar_DQL_Run_3_Actions</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/j5xjvzy9' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/j5xjvzy9</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250804_124927-j5xjvzy9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250804_125152-xvd84yen</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/xvd84yen' target=\"_blank\">MountainCar_DQL_Run_3_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/xvd84yen' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/xvd84yen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/xvd84yen?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7de967946f10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions, learning_rate_a=learning_rate_a, discount_factor_g=discount_factor_g, seed=seed, lr_step_size=lr_step_size)\n",
        "\n",
        "mountaincar.train(10000, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTds2tqvwwBX",
        "outputId": "e37bafd7-1d7d-4a54-b809-84becc04b1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_train_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best rewards so far: -187.9999999999998\n",
            "Episode 1000 Epsilon 0.5\n",
            "Best rewards so far: -147.69999999999922\n",
            "Best rewards so far: -139.59999999999945\n",
            "Best rewards so far: -75.69999999999965\n",
            "Episode 2000 Epsilon 0.3333333333333333\n",
            "Episode 3000 Epsilon 0.25\n",
            "Best rewards so far: -38.09999999999974\n",
            "Best rewards so far: -34.89999999999981\n",
            "Episode 4000 Epsilon 0.2\n",
            "Best rewards so far: 6.800000000000082\n",
            "Episode 5000 Epsilon 0.16666666666666666\n",
            "Episode 6000 Epsilon 0.14285714285714285\n",
            "Episode 7000 Epsilon 0.125\n",
            "Episode 8000 Epsilon 0.1111111111111111\n",
            "Episode 9000 Epsilon 0.1\n",
            "Final model saved as mountaincar_dql_final.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8147d03",
        "outputId": "4d636deb-3742-48b8-dc7f-f0c731c607cf"
      },
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_4209.pt\n",
            "Episode 1: Goal Reached! Reward: 90.90000000000002\n",
            "Episode 2: Goal Reached! Reward: 90.30000000000003\n",
            "Episode 3: Goal Reached! Reward: 90.30000000000003\n",
            "Episode 4: Goal Reached! Reward: 90.90000000000002\n",
            "Episode 5: Goal Reached! Reward: 90.50000000000003\n",
            "Episode 6: Goal Reached! Reward: 90.90000000000002\n",
            "Episode 7: Goal Reached! Reward: 90.60000000000002\n",
            "Episode 8: Goal Reached! Reward: 90.70000000000002\n",
            "Episode 9: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 10: Goal Reached! Reward: 90.90000000000002\n",
            "Episode 11: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 12: Goal Reached! Reward: 90.70000000000002\n",
            "Episode 13: Goal Reached! Reward: 90.70000000000002\n",
            "Episode 14: Goal Reached! Reward: 90.90000000000002\n",
            "Episode 15: Goal Reached! Reward: 91.00000000000003\n",
            "Episode 16: Goal Reached! Reward: 90.40000000000002\n",
            "Episode 17: Goal Reached! Reward: 90.90000000000002\n",
            "Episode 18: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 19: Goal Reached! Reward: 90.80000000000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Goal Reached! Reward: 90.80000000000003\n",
            "Average test reward over 20 episodes: 90.73000000000002\n",
            "Logging 21 test videos to wandb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizzazione dello spazio delle azioni in 11 azioni"
      ],
      "metadata": {
        "id": "O5jO8tARgZAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kK93DcuggKV",
        "outputId": "258056f8-8d4a-4775-d952-f213e2fa4324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: mountaincar_dql_2222.pt\n",
            "Deleted: mountaincar_autosave_dql_7000.pt\n",
            "Deleted: mountaincar_autosave_dql_3000.pt\n",
            "Deleted: mountaincar_autosave_dql_4000.pt\n",
            "Deleted: mountaincar_autosave_dql_8000.pt\n",
            "Deleted: mountaincar_autosave_dql_6000.pt\n",
            "Deleted: mountaincar_dql_15010.pt\n",
            "Deleted: mountaincar_autosave_dql_19000.pt\n",
            "Deleted: mountaincar_dql_11636.pt\n",
            "Deleted: mountaincar_dql_12679.pt\n",
            "Deleted: mountaincar_dql_12363.pt\n",
            "Deleted: mountaincar_autosave_dql_9000.pt\n",
            "Deleted: mountaincar_autosave_dql_5000.pt\n",
            "Deleted: mountaincar_dql_238.pt\n",
            "Deleted: mountaincar_autosave_dql_18000.pt\n",
            "Deleted: mountaincar_autosave_dql_14000.pt\n",
            "Deleted: mountaincar_dql_6427.pt\n",
            "Deleted: mountaincar_autosave_dql_1000.pt\n",
            "Deleted: mountaincar_dql_15013.pt\n",
            "Deleted: mountaincar_dql_143.pt\n",
            "Deleted: mountaincar_dql_11534.pt\n",
            "Deleted: mountaincar_autosave_dql_13000.pt\n",
            "Deleted: mountaincar_autosave_dql_11000.pt\n",
            "Deleted: mountaincar_autosave_dql_16000.pt\n",
            "Deleted: mountaincar_autosave_dql_15000.pt\n",
            "Deleted: mountaincar_dql_65.pt\n",
            "Deleted: mountaincar_autosave_dql_2000.pt\n",
            "Deleted: mountaincar_autosave_dql_10000.pt\n",
            "Deleted: mountaincar_dql_19.pt\n",
            "Deleted: mountaincar_autosave_dql_12000.pt\n",
            "Deleted: mountaincar_dql_1.pt\n",
            "Deleted: mountaincar_autosave_dql_17000.pt\n",
            "Deleted: mountaincar_dql_16009.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "8c8490b9",
        "outputId": "66b9f275-ca1c-4ec9-a2f4-0236e2806fce"
      },
      "source": [
        "import wandb\n",
        "num_discrete_actions=11\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.998\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.9\n",
        "lr_step_size=500\n",
        "epsilon_decay_c1=1000\n",
        "epsilon_decay_c2=1000\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    \"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    \"epsilon_decay_c2\": epsilon_decay_c2\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_11_Actions\", config=hyperparameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-piras\u001b[0m (\u001b[33mmatteo-piras-universit-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250804_132637-48hy5z3h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/48hy5z3h' target=\"_blank\">MountainCar_DQL_Run_11_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/48hy5z3h' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/48hy5z3h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/48hy5z3h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c99097a3b10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions, learning_rate_a=learning_rate_a, discount_factor_g=discount_factor_g, seed=seed, lr_step_size=lr_step_size)\n",
        "\n",
        "mountaincar.train(10000, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIH6DcDHglbP",
        "outputId": "9d556e35-dfec-4f80-cd72-79c9a8779869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000 Epsilon 0.5\n",
            "Best rewards so far: -149.50400078153513\n",
            "Best rewards so far: -147.8160009994496\n",
            "Best rewards so far: -145.06800086688898\n",
            "Best rewards so far: -126.18800068330683\n",
            "Best rewards so far: -68.65200082826573\n",
            "Episode 2000 Epsilon 0.3333333333333333\n",
            "Best rewards so far: -59.98000043153712\n",
            "Best rewards so far: -19.408000061034954\n",
            "Episode 3000 Epsilon 0.25\n",
            "Best rewards so far: -1.6880000276563862\n",
            "Episode 4000 Epsilon 0.2\n",
            "Episode 5000 Epsilon 0.16666666666666666\n",
            "Episode 6000 Epsilon 0.14285714285714285\n",
            "Episode 7000 Epsilon 0.125\n",
            "Episode 8000 Epsilon 0.1111111111111111\n",
            "Episode 9000 Epsilon 0.1\n",
            "Final model saved as mountaincar_dql_final.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ8o7UO1viLj",
        "outputId": "0e026d39-4767-44eb-cb0a-701a7502f1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_3355.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_test_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Goal Reached! Reward: 91.27599997425081\n",
            "Episode 2: Goal Reached! Reward: 91.90399996852877\n",
            "Episode 3: Goal Reached! Reward: 91.90399996852877\n",
            "Episode 4: Goal Reached! Reward: 93.54799990844728\n",
            "Episode 5: Goal Reached! Reward: 91.01199997711184\n",
            "Episode 6: Goal Reached! Reward: 91.17599997425081\n",
            "Episode 7: Goal Reached! Reward: 93.89999992847443\n",
            "Episode 8: Goal Reached! Reward: 91.63999997138978\n",
            "Episode 9: Goal Reached! Reward: 91.11199997711184\n",
            "Episode 10: Goal Reached! Reward: 91.27599997425081\n",
            "Episode 11: Goal Reached! Reward: 90.91199997711183\n",
            "Episode 12: Goal Reached! Reward: 90.91199997711183\n",
            "Episode 13: Goal Reached! Reward: 90.91199997711183\n",
            "Episode 14: Goal Reached! Reward: 93.54799990844728\n",
            "Episode 15: Goal Reached! Reward: 91.27599997425081\n",
            "Episode 16: Goal Reached! Reward: 91.01199997711184\n",
            "Episode 17: Goal Reached! Reward: 91.37599997425082\n",
            "Episode 18: Goal Reached! Reward: 91.17599997425081\n",
            "Episode 19: Goal Reached! Reward: 90.91199997711183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Goal Reached! Reward: 90.91199997711183\n",
            "Average test reward over 20 episodes: 91.5847999658108\n",
            "Logging 20 test videos to wandb.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizzazione dello spazio delle azioni in 101 azioni"
      ],
      "metadata": {
        "id": "rQPfnUZZWd_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmYjQxxXWhNQ",
        "outputId": "43704769-102a-483e-e093-df7b745a4f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: mountaincar_dql_2051.pt\n",
            "Deleted: mountaincar_dql_2662.pt\n",
            "Deleted: mountaincar_dql_5429.pt\n",
            "Deleted: mountaincar_dql_2052.pt\n",
            "Deleted: mountaincar_dql_3643.pt\n",
            "Deleted: mountaincar_dql_3645.pt\n",
            "Deleted: mountaincar_dql_1260.pt\n",
            "Deleted: mountaincar_dql_7826.pt\n",
            "Deleted: mountaincar_dql_4210.pt\n",
            "Deleted: mountaincar_dql_2664.pt\n",
            "Deleted: mountaincar_dql_1263.pt\n",
            "Deleted: mountaincar_dql_1741.pt\n",
            "Deleted: mountaincar_dql_3644.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "num_discrete_actions=101\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.998\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.9\n",
        "lr_step_size=1000\n",
        "epsilon_decay_c1=1500\n",
        "epsilon_decay_c2=1500\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    \"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    \"epsilon_decay_c2\": epsilon_decay_c2\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_101_Actions\", config=hyperparameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "5YYmPBe3Wky8",
        "outputId": "66ef5237-3408-45e0-cb44-be270c7efc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>██▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward_per_episode</td><td>▆▆▅▆▆▅▆▆▅▅▆▇▆▆▆▇▅▅▇▆▆██▇█▄█▇▇▂██▃▆▅▄▇▆█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.51867</td></tr><tr><td>learning_rate</td><td>0.01</td></tr><tr><td>reward_per_episode</td><td>-1065.50008</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MountainCar_DQL_Run_101_Actions</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/ljlx94wt' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/ljlx94wt</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250804_142425-ljlx94wt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250804_142656-29czh1vc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/29czh1vc' target=\"_blank\">MountainCar_DQL_Run_101_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/29czh1vc' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/29czh1vc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/29czh1vc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c98ceaa1ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions,\n",
        "                             learning_rate_a=learning_rate_a,\n",
        "                             discount_factor_g=discount_factor_g,\n",
        "                             seed=seed,\n",
        "                             lr_step_size=lr_step_size,\n",
        "                             epsilon_decay_c1=epsilon_decay_c1,\n",
        "                             epsilon_decay_c2=epsilon_decay_c2)\n",
        "\n",
        "mountaincar.train(10000, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVwiCE3oW2oW",
        "outputId": "8b6960a2-fdf7-481d-9fdb-87ebd6ab4f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_train_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:432: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
            "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000 Epsilon 0.6\n",
            "Best rewards so far: -135.560718611507\n",
            "Best rewards so far: -94.50891987379069\n",
            "Best rewards so far: -73.26427989575413\n",
            "Episode 2000 Epsilon 0.42857142857142855\n",
            "Best rewards so far: -53.049799330186616\n",
            "Episode 3000 Epsilon 0.3333333333333333\n",
            "Best rewards so far: -34.473519697170275\n",
            "Episode 4000 Epsilon 0.2727272727272727\n",
            "Best rewards so far: -28.979999962091185\n",
            "Best rewards so far: -25.614199138450687\n",
            "Best rewards so far: 13.264360595836678\n",
            "Episode 5000 Epsilon 0.23076923076923078\n",
            "Episode 6000 Epsilon 0.2\n",
            "Episode 7000 Epsilon 0.17647058823529413\n",
            "Episode 8000 Epsilon 0.15789473684210525\n",
            "Episode 9000 Epsilon 0.14285714285714285\n",
            "Best rewards so far: 15.9392798959923\n",
            "Final model saved as mountaincar_dql_final.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjXuXu32XFCm",
        "outputId": "a7e63441-3544-4fe1-8a1c-c023de729d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_9088.pt\n",
            "Episode 1: Goal Reached! Reward: 94.50755988639831\n",
            "Episode 2: Goal Reached! Reward: 94.51519986801148\n",
            "Episode 3: Goal Reached! Reward: 94.51519986801148\n",
            "Episode 4: Goal Reached! Reward: 94.52479987258911\n",
            "Episode 5: Goal Reached! Reward: 94.3315599092865\n",
            "Episode 6: Goal Reached! Reward: 94.50755988639831\n",
            "Episode 7: Goal Reached! Reward: 94.52479987258911\n",
            "Episode 8: Goal Reached! Reward: 94.57987987815856\n",
            "Episode 9: Goal Reached! Reward: 94.46659988822937\n",
            "Episode 10: Goal Reached! Reward: 94.54851988456726\n",
            "Episode 11: Goal Reached! Reward: 94.43523989463806\n",
            "Episode 12: Goal Reached! Reward: 94.39427989646911\n",
            "Episode 13: Goal Reached! Reward: 94.40387990104675\n",
            "Episode 14: Goal Reached! Reward: 94.52479987258911\n",
            "Episode 15: Goal Reached! Reward: 94.53891987998962\n",
            "Episode 16: Goal Reached! Reward: 94.29059991111755\n",
            "Episode 17: Goal Reached! Reward: 94.53891987998962\n",
            "Episode 18: Goal Reached! Reward: 94.46659988822937\n",
            "Episode 19: Goal Reached! Reward: 94.43523989463806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Goal Reached! Reward: 94.43523989463806\n",
            "Average test reward over 20 episodes: 94.47426988637923\n",
            "Logging 21 test videos to wandb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizzazione dello spazio delle azioni in 1001 azioni"
      ],
      "metadata": {
        "id": "hz12MjgxkcHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0JYdHbfkfji",
        "outputId": "93ca10d7-f482-455e-c070-18272507f43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: mountaincar_dql_4781.pt\n",
            "Deleted: mountaincar_dql_4144.pt\n",
            "Deleted: mountaincar_dql_3238.pt\n",
            "Deleted: mountaincar_dql_2558.pt\n",
            "Deleted: mountaincar_dql_1593.pt\n",
            "Deleted: mountaincar_dql_1552.pt\n",
            "Deleted: mountaincar_dql_1336.pt\n",
            "Deleted: mountaincar_dql_4929.pt\n",
            "Deleted: mountaincar_dql_9088.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "num_discrete_actions=1001\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.996\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.95\n",
        "lr_step_size=500\n",
        "#epsilon_decay_c1=2000\n",
        "#epsilon_decay_c2=2000\n",
        "epsilon_decay_rate=0.00015\n",
        "epsilon_min=0.01\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    #\"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    #\"epsilon_decay_c2\": epsilon_decay_c2,\n",
        "    \"epsilon_decay_rate\": epsilon_decay_rate,\n",
        "    \"epsilon_min\": epsilon_min\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_1001_Actions\", config=hyperparameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "YIBcovVAkt_P",
        "outputId": "c4c8236a-7a62-410e-af70-426ddf21ecbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>█▇▇▇▇▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>█▇▇▆▆▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward_per_episode</td><td>▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂█▂▂▂▁▂▁▁▁▁▁▁▁▁▂▂▂▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.24005</td></tr><tr><td>learning_rate</td><td>0.00047</td></tr><tr><td>reward_per_episode</td><td>-1008.85984</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MountainCar_DQL_Run_1001_Actions</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/axwa9a90' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/axwa9a90</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250805_075212-axwa9a90/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250805_082008-qhdj4s8o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/qhdj4s8o' target=\"_blank\">MountainCar_DQL_Run_1001_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/qhdj4s8o' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/qhdj4s8o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/qhdj4s8o?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x79d7eb78db50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions,\n",
        "                             learning_rate_a=learning_rate_a,\n",
        "                             discount_factor_g=discount_factor_g,\n",
        "                             seed=seed,\n",
        "                             lr_step_size=lr_step_size,\n",
        "                             #epsilon_decay_c1=epsilon_decay_c1,\n",
        "                             #epsilon_decay_c2=epsilon_decay_c2,\n",
        "                             epsilon_decay_rate=epsilon_decay_rate,\n",
        "                             epsilon_min=epsilon_min\n",
        "                             )\n",
        "\n",
        "mountaincar.train(20000, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "7Ar58hoMky92",
        "outputId": "9ec266a1-8a09-4c74-a5db-ef95e84e480d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_train_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000 Epsilon 0.8622287213819209\n",
            "Episode 2000 Epsilon 0.7435200582319474\n",
            "Episode 3000 Epsilon 0.64134656498801\n",
            "Episode 4000 Epsilon 0.5534050243737413\n",
            "Episode 5000 Epsilon 0.47771303890793215\n",
            "Best rewards so far: -153.18339632413935\n",
            "Episode 6000 Epsilon 0.4125643432660606\n",
            "Episode 7000 Epsilon 0.3564903412734134\n",
            "Episode 8000 Epsilon 0.30822700048826734\n",
            "Episode 9000 Epsilon 0.26668635810557134\n",
            "Episode 10000 Epsilon 0.230931995860964\n",
            "Episode 11000 Epsilon 0.20015793108503963\n",
            "Episode 12000 Epsilon 0.17367044806538\n",
            "Episode 13000 Epsilon 0.15087246015493574\n",
            "Episode 14000 Epsilon 0.13125005011397434\n",
            "Episode 15000 Epsilon 0.11436088527503568\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3564783730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                              )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmountaincar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1587791555.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes, render)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0;31m# Use the continuous state as input and get the index of the best discrete action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                         \u001b[0maction_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_to_dqn_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Map the discrete action index to the continuous action value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4173739157.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz2ksG0HqtvD",
        "outputId": "a6516170-95f6-48c9-da74-c0ccb142d89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_9504.pt\n",
            "Episode 1: Goal Reached! Reward: 86.58709481586745\n",
            "Episode 2: Goal Reached! Reward: 93.21152286854395\n",
            "Episode 3: Goal Reached! Reward: 93.21266527670282\n",
            "Episode 4: Goal Reached! Reward: 93.01287928854937\n",
            "Episode 5: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 6: Goal Reached! Reward: 86.6001073337561\n",
            "Episode 7: Goal Reached! Reward: 93.07173048098444\n",
            "Episode 8: Goal Reached! Reward: 93.25663483342585\n",
            "Episode 9: Goal Reached! Reward: 86.71916257544083\n",
            "Episode 10: Goal Reached! Reward: 86.44480716333447\n",
            "Episode 11: Goal Reached! Reward: 86.47372538655533\n",
            "Episode 12: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 13: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 14: Goal Reached! Reward: 93.01287928854937\n",
            "Episode 15: Goal Reached! Reward: 92.6212885493186\n",
            "Episode 16: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 17: Goal Reached! Reward: 92.97266789948571\n",
            "Episode 18: Goal Reached! Reward: 86.71916257544083\n",
            "Episode 19: Goal Reached! Reward: 86.41487419412026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Goal Reached! Reward: 86.41487419412026\n",
            "Average test reward over 20 episodes: 55.43699990020466\n",
            "Logging 21 test videos to wandb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a92aad1"
      },
      "source": [
        "# Ulteriori tentativi per la quantizzazione in 1001 azioni\n",
        "Come ulteriore tentativo per ottenere una buona policy per il caso di 1001 azioni ho provato a modificare la Rete aumentando la dimensione dell'hidden layer e inoltre ho provato a fare reward shaping aggiungendo un reward positivo ogni volta che l'agente ragienge una nuova posizione a destra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_actions, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(100, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(500, num_actions)\n",
        "            )\n",
        "\n",
        "        # Initialize FC layer weights using He initialization\n",
        "        for layer in [self.FC]:\n",
        "            for module in layer:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.FC(x)\n",
        "        return Q\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "aeAR4IB2pUiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6232521f"
      },
      "source": [
        "class MountainCarDQL():\n",
        "\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error.\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    def __init__(self, learning_rate_a=75e-5, discount_factor_g=0.96, network_sync_rate=100, replay_memory_size=100000, mini_batch_size=64, num_discrete_actions=10, seed=None, lr_decay_gamma=0.9, lr_step_size=1000, epsilon_decay_c1=1000, epsilon_decay_c2=1000, epsilon_decay_rate=0.0001, epsilon_min=0.01):\n",
        "        self.learning_rate_a = learning_rate_a\n",
        "        self.discount_factor_g = discount_factor_g\n",
        "        self.network_sync_rate = network_sync_rate\n",
        "        self.replay_memory_size = replay_memory_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.num_discrete_actions = num_discrete_actions\n",
        "        self.seed = seed\n",
        "        self.lr_decay_gamma = lr_decay_gamma # learning rate decay\n",
        "        self.lr_step_size = lr_step_size     # learning rate decay\n",
        "        self.epsilon_decay_c1 = epsilon_decay_c1 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_c2 = epsilon_decay_c2 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate # exponential epsilon decay\n",
        "        self.epsilon_min = epsilon_min # minimum epsilon value\n",
        "\n",
        "        self.linearDecay = False\n",
        "        self.hyperbolicDecay = False\n",
        "        self.exponentialDecay = True\n",
        "\n",
        "        if self.seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(self.seed)\n",
        "            torch.manual_seed(self.seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(self.seed)\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        # Initialize max_position to the minimum possible position\n",
        "        # MountainCarContinuous-v0 has observation_space.low[0] as the minimum position\n",
        "        # Initialize it here, but the actual environment needs to be created first to get the low value.\n",
        "        # Let's initialize to None and get the value from the env inside train\n",
        "        self.max_position = None\n",
        "\n",
        "\n",
        "    # Train the environment\n",
        "    def train(self, episodes, render=False):\n",
        "        # Create MountainCarContinuous instance\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_train_video', episode_trigger=lambda x: x % 1000 == 0) # Record every 1000 episodes during training\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Initialize max_position if it's None (first time training)\n",
        "        if self.max_position is None:\n",
        "             self.max_position = env.observation_space.low[0]\n",
        "\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "        epsilon = 1 # Initial epsilon\n",
        "\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        target_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # Learning rate scheduler - learning rate decay\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.lr_step_size, gamma=self.lr_decay_gamma)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "        best_rewards=-200 # Adjusted initial best_rewards for continuous env\n",
        "        goal_reached=False\n",
        "\n",
        "        for i in range(episodes):\n",
        "\n",
        "            self.max_position = env.observation_space.low[0]\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent reached goal\n",
        "            truncated = False\n",
        "\n",
        "            rewards = 0\n",
        "\n",
        "            # Agent navigates map until it reaches goal (terminated), or is truncated.\n",
        "            while(not terminated and not truncated):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action (index for discrete actions) uniformly\n",
        "                    action_index = random.randrange(self.num_discrete_actions)\n",
        "\n",
        "                else:\n",
        "                    # select best action (index for discrete actions)\n",
        "                    with torch.no_grad():\n",
        "                        # Use the continuous state as input and get the index of the best discrete action\n",
        "                        action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                new_state,reward,terminated,truncated,_ = env.step([action])\n",
        "\n",
        "                # Add a small negative reward at each timestep to discourage staying in the valley\n",
        "                reward -= 1\n",
        "\n",
        "                # Add positive reward for reaching new maximum position\n",
        "                if new_state[0] > self.max_position:\n",
        "                    reward += 10.0 # Add positive reward for progress\n",
        "                    self.max_position = new_state[0] # Update max position\n",
        "\n",
        "                # Accumulate reward\n",
        "                rewards += reward\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action_index, new_state, reward, terminated)) # Store action_index, not continuous action value\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(rewards)\n",
        "\n",
        "            # Log reward per episode to wandb\n",
        "            wandb.log({\"reward_per_episode\": rewards}, step=i)\n",
        "\n",
        "\n",
        "            # Check if goal was reached\n",
        "            if(terminated):\n",
        "                goal_reached = True\n",
        "\n",
        "            # Graph training progress\n",
        "            #if(i!=0 and i%1000==0):\n",
        "                #print(f'Episode {i} Epsilon {epsilon}')\n",
        "\n",
        "                #self.plot_progress(rewards_per_episode, epsilon_history)\n",
        "                #torch.save(policy_dqn.state_dict(), f\"mountaincar_autosave_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            if rewards>best_rewards:\n",
        "                best_rewards = rewards\n",
        "                print(f'Best rewards so far: {best_rewards}')\n",
        "                # Save policy\n",
        "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            # Check if enough experience has been collected AND goal was reached\n",
        "            if len(memory)>self.mini_batch_size and goal_reached:\n",
        "                mini_batch = memory.sample(self.mini_batch_size) # Use mini_batch_size for sampling\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                if(self.linearDecay):\n",
        "                  epsilon = max(epsilon - 1/episodes, self.epsilon_min)\n",
        "                elif(self.hyperbolicDecay):\n",
        "                  epsilon = max(self.epsilon_decay_c1 / (self.epsilon_decay_c2 + i), self.epsilon_min)\n",
        "                elif(self.exponentialDecay):\n",
        "                  epsilon = self.epsilon_min + (1 - self.epsilon_min) * math.exp(-self.epsilon_decay_rate * i)\n",
        "\n",
        "                epsilon_history.append(epsilon)\n",
        "                # Log epsilon to wandb\n",
        "                wandb.log({\"epsilon\": epsilon}, step=i)\n",
        "\n",
        "                # Step the learning rate scheduler - learning rate decay\n",
        "                self.scheduler.step()\n",
        "                # Log current learning rate to wandb\n",
        "                wandb.log({\"learning_rate\": self.optimizer.param_groups[0]['lr']}, step=i)\n",
        "\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Save the final model\n",
        "        #torch.save(policy_dqn.state_dict(), \"mountaincar_dql_final.pt\")\n",
        "        #print(\"Final model saved as mountaincar_dql_final.pt\")\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "    #def plot_progress(self, rewards_per_episode, epsilon_history):\n",
        "        # Create new graph\n",
        "        #plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
        "        # for x in range(len(rewards_per_episode)):\n",
        "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
        "        #plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.plot(sum_rewards)\n",
        "        #plt.plot(rewards_per_episode)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        #plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        #plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        #plt.savefig('mountaincar_dql.png')\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action_index, new_state, reward, terminated in mini_batch: # Use action_index\n",
        "\n",
        "            if terminated:\n",
        "                # Agent receive reward of 100 for reaching goal.\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            # Use the continuous state as input\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            # Use the continuous state as input\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
        "            # Adjust the specific action (index) to the target that was just calculated\n",
        "            target_q[action_index] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts a state (position, velocity) to tensor representation for continuous observation space.\n",
        "    Example:\n",
        "    Input = (0.3, -0.03)\n",
        "    Return = tensor([0.3, -0.03])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
        "        # The state is already a NumPy array [position, velocity]\n",
        "        # Convert it directly to a PyTorch FloatTensor\n",
        "        return torch.FloatTensor(state)\n",
        "\n",
        "    # Run the environment with the learned policy\n",
        "    def test(self, episodes, model_filepath):\n",
        "\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_test_video', episode_trigger=lambda x: True) # Record every episode\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "\n",
        "        num_states = env.observation_space.shape[0]\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        total_test_rewards = 0\n",
        "        test_rewards_list = []\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            rewards = 0\n",
        "\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action (index)\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input and get the index of the best discrete action\n",
        "                    action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                state,reward,terminated,truncated,_ = env.step([action])\n",
        "                rewards += reward\n",
        "\n",
        "            total_test_rewards += rewards\n",
        "            test_rewards_list.append(rewards)\n",
        "\n",
        "            # Check if the goal was reached (terminated without truncation)\n",
        "            # MountainCarContinuous-v0 terminates when the flag is reached\n",
        "            if terminated:\n",
        "                print(f\"Episode {i+1}: Goal Reached! Reward: {rewards}\")\n",
        "            elif truncated:\n",
        "                print(f\"Episode {i+1}: Episode truncated (did not reach goal). Reward: {rewards}\")\n",
        "            else: # This case should not happen in MountainCarContinuous if not truncated\n",
        "                 print(f\"Episode {i+1}: Episode terminated unexpectedly. Reward: {rewards}\")\n",
        "\n",
        "        # Calculate and Log average test reward to wandb\n",
        "        if episodes > 0:\n",
        "            avg_test_reward = total_test_rewards / episodes\n",
        "            wandb.log({\"average_test_reward\": avg_test_reward})\n",
        "            print(f\"Average test reward over {episodes} episodes: {avg_test_reward}\")\n",
        "        else:\n",
        "            print(\"No test episodes run.\")\n",
        "\n",
        "        # Log test videos to wandb\n",
        "        # Assuming videos are saved in 'mountaincar_test_video' directory\n",
        "        # Wandb can log video files directly.\n",
        "        # We need to find the video files generated during this test run.\n",
        "        # The RecordVideo wrapper names videos based on the episode index.\n",
        "        video_files = glob.glob('mountaincar_test_video/rl-video-episode-*.mp4')\n",
        "        if video_files:\n",
        "            print(f\"Logging {len(video_files)} test videos to wandb.\")\n",
        "            for video_file in video_files:\n",
        "                wandb.log({\"test_video\": wandb.Video(video_file)})\n",
        "        else:\n",
        "            print(\"No test videos found to log.\")\n",
        "\n",
        "\n",
        "        env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "num_discrete_actions=1001\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.998\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.95\n",
        "lr_step_size=500\n",
        "#epsilon_decay_c1=2000\n",
        "#epsilon_decay_c2=2000\n",
        "epsilon_decay_rate=0.00015\n",
        "epsilon_min=0.01\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    #\"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    #\"epsilon_decay_c2\": epsilon_decay_c2,\n",
        "    \"epsilon_decay_rate\": epsilon_decay_rate,\n",
        "    \"epsilon_min\": epsilon_min\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_1001_Actions\", config=hyperparameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "uO1dLIixc-VK",
        "outputId": "3fce2755-612f-4c3f-8cf5-c4905f1a3766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-piras\u001b[0m (\u001b[33mmatteo-piras-universit-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250805_134617-dm4vsut1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/dm4vsut1' target=\"_blank\">MountainCar_DQL_Run_1001_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/dm4vsut1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/dm4vsut1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/dm4vsut1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c379ec73850>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19e1d454",
        "outputId": "1550a3b7-1b85-47c9-d90f-5b4f9dce7e73"
      },
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions,\n",
        "                             learning_rate_a=learning_rate_a,\n",
        "                             discount_factor_g=discount_factor_g,\n",
        "                             seed=seed,\n",
        "                             lr_step_size=lr_step_size,\n",
        "                             epsilon_decay_rate=epsilon_decay_rate,\n",
        "                             epsilon_min=epsilon_min\n",
        "                             )\n",
        "\n",
        "mountaincar.train(20000, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best rewards so far: -59.8872571879049\n",
            "Best rewards so far: -1.6784395974440658\n",
            "Best rewards so far: 129.3702262224496\n",
            "Best rewards so far: 288.0186160275843\n",
            "Best rewards so far: 328.0757776474799\n",
            "Episode 1000 Epsilon 1\n",
            "Episode 2000 Epsilon 0.7435200582319474\n",
            "Episode 3000 Epsilon 0.64134656498801\n",
            "Episode 4000 Epsilon 0.5534050243737413\n",
            "Episode 5000 Epsilon 0.47771303890793215\n",
            "Episode 6000 Epsilon 0.4125643432660606\n",
            "Episode 7000 Epsilon 0.3564903412734134\n",
            "Episode 8000 Epsilon 0.30822700048826734\n",
            "Episode 9000 Epsilon 0.26668635810557134\n",
            "Episode 10000 Epsilon 0.230931995860964\n",
            "Episode 11000 Epsilon 0.20015793108503963\n",
            "Episode 12000 Epsilon 0.17367044806538\n",
            "Episode 13000 Epsilon 0.15087246015493574\n",
            "Episode 14000 Epsilon 0.13125005011397434\n",
            "Episode 15000 Epsilon 0.11436088527503568\n",
            "Episode 16000 Epsilon 0.09982424638300359\n",
            "Episode 17000 Epsilon 0.08731244533822084\n",
            "Episode 18000 Epsilon 0.07654343837953297\n",
            "Episode 19000 Epsilon 0.06727446819201335\n",
            "Final model saved as mountaincar_dql_final.pt\n"
          ]
        }
      ]
    }
  ]
}